# train.py
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoTokenizer

from config import BERT_PRETRAINED_MODEL_NAME, HUGGINGFACE_CACHEDIR,TOKEN_MAX_LENGTH
from dataset import ThemeStartDataset
from models import ThemeStartPredictor
from utils import load_texts_by_folder

# è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
max_length = TOKEN_MAX_LENGTH
# åˆå§‹åŒ– tokenizer
tokenizer = AutoTokenizer.from_pretrained(BERT_PRETRAINED_MODEL_NAME, cache_dir=HUGGINGFACE_CACHEDIR)

# æ„é€ è®­ç»ƒæ ·æœ¬
train_seqs = [
]
train_seqs.extend(load_texts_by_folder("./data/train",endswidth=".md"))
# æ„é€  Dataset å’Œ DataLoader
dataset = ThemeStartDataset(train_seqs, tokenizer, max_length=max_length)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# æ¨¡å‹ã€lossã€ä¼˜åŒ–å™¨
model = ThemeStartPredictor().to(device)
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)  # æ¯” Adam æ›´ç¨³å®šï¼ˆå¸¦æƒé‡è¡°å‡ï¼‰

# è®­ç»ƒæ¨¡å¼
model.train()

max_batches = 100
batch_count = 0

while batch_count < max_batches:
    for batch in dataloader:
        if batch_count >= max_batches:
            break

        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["start_index"].to(device)

        optimizer.zero_grad()
        _, logits = model(input_ids, attention_mask)  # logits shape: (batch, seq_len)
        loss = loss_fn(logits, labels)  # labels shape: (batch,)

        loss.backward()
        optimizer.step()

        print(f"Batch {batch_count + 1:03d} | Loss: {loss.item():.4f}")
        batch_count += 1

model_path = "weights/theme_start_model.pt"
torch.save(model.state_dict(), model_path)
print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

# âœ… åŠ è½½æ¨¡å‹ & æ¨ç†æµ‹è¯•
print("\nğŸ” åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œæµ‹è¯•é¢„æµ‹ï¼š")
model.load_state_dict(torch.load(model_path))
model.eval()

# æµ‹è¯•å¥å­
test_text = load_texts_by_folder("./data/test",endswidth=".md")[0]

# å»é™¤ <theme_start> å tokenize
clean_text = test_text.replace("<ai-theme>", "")

encoding = tokenizer(
    clean_text,
    return_offsets_mapping=True,
    padding='max_length',
    truncation=True,
    max_length=max_length,
    return_tensors='pt'
)

input_ids = encoding["input_ids"].to(device)
attention_mask = encoding["attention_mask"].to(device)
offsets = encoding["offset_mapping"][0]

with torch.no_grad():
    pred_index, _ = model(input_ids, attention_mask)
    pred_index = pred_index.item()

# æ‰“å°é¢„æµ‹ç»“æœ
char_start = offsets[pred_index][0].item()
print(f"\nğŸ§ª åŸå§‹æ–‡æœ¬: \n{test_text[:256]}")
print(f"ğŸ“ æ¨¡å‹é¢„æµ‹ä¸»è¦å†…å®¹å¼€å§‹ä½ç½® token_index: {pred_index}")
print(f"ğŸ“ å¯¹åº”æ–‡æœ¬å†…å®¹: \n{clean_text[char_start:char_start+125]}")