import torch
from transformers import AutoTokenizer
from config import BERT_PRETRAINED_MODEL_NAME, HUGGINGFACE_CACHEDIR, TOKEN_MAX_LENGTH
from models import ThemeStartPredictor

def load_model(model_path: str, device):
    """
    åŠ è½½æ¨¡å‹å¹¶è®¾ç½®ä¸º eval æ¨¡å¼
    """
    model = ThemeStartPredictor().to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    return model

def load_tokenizer():
    """
    åŠ è½½ tokenizer
    """
    return AutoTokenizer.from_pretrained(BERT_PRETRAINED_MODEL_NAME, cache_dir=HUGGINGFACE_CACHEDIR)

def predict_theme_start(text: str, model, tokenizer, device, max_length=TOKEN_MAX_LENGTH) -> int:
    """
    é¢„æµ‹ä¸»é¢˜å¼€å§‹ä½ç½®ï¼ˆå­—ç¬¦çº§ç´¢å¼•ï¼‰

    :param text: åŸå§‹çº¯æ–‡æœ¬ï¼ˆä¸å« <ai-theme> æ ‡ç­¾ï¼‰
    :param model: å·²åŠ è½½å¹¶è®¾ç½®ä¸º eval() çš„æ¨¡å‹
    :param tokenizer: å¯¹åº”çš„ tokenizer
    :param device: è¿è¡Œè®¾å¤‡ï¼ˆcuda / cpuï¼‰
    :param max_length: æœ€å¤§ token é•¿åº¦
    :return: å­—ç¬¦ç´¢å¼•ä½ç½®ï¼ˆchar_startï¼‰
    """
    encoding = tokenizer(
        text,
        return_offsets_mapping=True,
        padding="max_length",
        truncation=True,
        max_length=max_length,
        return_tensors="pt"
    )

    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)
    offsets = encoding["offset_mapping"][0]

    with torch.no_grad():
        pred_index, _ = model(input_ids, attention_mask)
        pred_index = pred_index.item()

    char_start = offsets[pred_index][0].item()
    return char_start

# ğŸ§ª ç¤ºä¾‹è¿è¡Œæ–¹å¼ï¼ˆå¯é€‰ï¼‰
if __name__ == "__main__":
    import argparse
    import os

    parser = argparse.ArgumentParser(description="ä¸»é¢˜å¼€å§‹ä½ç½®é¢„æµ‹")
    parser.add_argument("--model_path", type=str, default="weights/theme_start_model.pt", help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--text", type=str, help="å¾…é¢„æµ‹çš„æ–‡æœ¬å†…å®¹ï¼ˆä¼˜å…ˆäºæ–‡ä»¶ï¼‰")
    parser.add_argument("--file", type=str, help="åŒ…å«å¾…é¢„æµ‹æ–‡æœ¬çš„æ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    if not args.text and not args.file:
        print("âŒ é”™è¯¯ï¼šå¿…é¡»æä¾› --text æˆ– --file å‚æ•°ä¹‹ä¸€")
        exit(1)

    if args.text:
        text = args.text
    else:
        if not os.path.isfile(args.file):
            print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
            exit(1)
        with open(args.file, 'r', encoding='utf-8') as f:
            text = f.read().strip()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = load_tokenizer()
    model = load_model(args.model_path, device)

    start_index = predict_theme_start(text, model, tokenizer, device)
    print("ğŸ“ é¢„æµ‹å­—ç¬¦èµ·å§‹ä½ç½®:", start_index)
    print("ğŸ“ é¢„æµ‹ç‰‡æ®µ:", text[start_index:start_index + 125])
